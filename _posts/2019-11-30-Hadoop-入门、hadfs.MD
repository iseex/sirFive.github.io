---
layout: post
title: "Hadoop介绍01"
date: 2019-11-30
description: "hadoop历史、hadoop存储模型、架构模型、读写流程、伪分布式安装"
tag: Hadoop
---
### 1.hadoop历史
#### 1.Hadoop要解决的问题
#### 1.问题：1台内存只有1G的计算机，如何在1个1个1T的文件查找重复的两行？
解决办法：由于计算机内存不能放下全量数据，因此计算机每次处理1行，并求hash值，hash值相同则说明两行相同。
求hash半小时，排序半小时。  
改进1：每次处理1万行（假设1万行的总大小还是小于1G），并求hash。假设每次处理时间为2秒，则总的处理时间为
2*(1T/1万行的文件大小)（相比前面减少IO次数）。  
改进2-大数据的处理方法：使用一个(1T/1万行的文件大小)分布式集群进行处理，则耗时为2秒，时间大大减少。
#### 2.总结
问题中涉及到的技术包括：1.并行-提升速度的关键。2.分布式。3.计算和数据在同一台机器。4.文件的切割管理。  
Hadoop所要解决的就是：分布式文件管理、分布式计算、将算法向数据移动（相比要处理的数据，算法要小的多）、管理和规范化操作。
### 2.hadoop存储模型
Hadoop的存储模型：字节。  
Hadoop将文件线性的切割成块（block），block分散存储在不同的集群节点中，并通过偏移量（offset）即索引下标计算文件的位置。  
特点：一个文件切割成的block块大小要相同（除最后一块外），不同的文件切割成的block块大小可以不一样。  
block可以设置副本数，副本无序散列在不同的节点中（设置副本主要是为了安全，比如某个服务器挂了又不设置副本，就会造成在这个
节点上的block会永久的丢失）。注意副本数不要超过节点数量，原因：一个节点上放两个相同的block无意义。  
文件上传可以设置block大小（系统默认1M~128M）和副本数）和副本数；已经上传的文件block副本数可以调整，但是
大小不能改变，大小改变会造成所有的索引都会变。  
只支持一次写入多次读取，同一时刻只有一个写入者。  
可以append追加数据。
### 3.hadoop架构模型
架构模型-主从架构  
文件元数据MetaData，文件数据:元数据\数据本身。  
（主）NameNode节点保存文件元数据(文件名、文件大小、偏移量等)：单节点、posix等。  
（从）DataNode节点保存文件Block数据（具体的数据）：多节点存储。  
DataNode与NameNode保持心跳，DataNode向NameNode提交Block列表。  
HdfsClient与NameNode交互元数据信息。  
HdfsClient与DataNode交互文件Block数据（cs模式）。  
DataNode利用服务器本地文件系统存储数据块。  
